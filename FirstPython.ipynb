{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FirstPython.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "djfaMpQI05gz",
        "1BcKXXVm3pXn"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfJx29wuzTJE",
        "colab_type": "text"
      },
      "source": [
        "# **Imports and settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3AnEfbshjQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from shutil import copyfile\n",
        "from shutil import rmtree\n",
        "import logging\n",
        "import json\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "from more_itertools import sliced\n",
        "from random import randint\n",
        "from random import choices\n",
        "import random\n",
        "from google.cloud import storage\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split  \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "logging.basicConfig(filename=\"tracker.log\", level=logging.DEBUG, filemode=\"w\")\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"activitytracker-bde5c-firebase-adminsdk-ks82x-ad642b2476.json\"\n",
        "client = storage.Client()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH3laz0dz88Z",
        "colab_type": "text"
      },
      "source": [
        "# **Data Types**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg5ogUj1A9Iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Motion:\n",
        "    def __init__(self, name: str, tag: str):\n",
        "        self.name = name\n",
        "        self.tag = tag\n",
        "\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, dict_json: dict):\n",
        "        self.fileName = dict_json['fileName']\n",
        "        self.motion = dict_json['motion']\n",
        "        self.time = dict_json['time']\n",
        "        self.userName = dict_json['userName']\n",
        "        self.listX = []\n",
        "        self.listY = []\n",
        "        self.listZ = []\n",
        "        \n",
        "        for idx, val in enumerate(dict_json['graphList']):\n",
        "            if idx % 3 == 0:\n",
        "                self.listX.append(val)\n",
        "            if idx % 3 == 1:\n",
        "                self.listY.append(val)\n",
        "            if idx % 3 == 2:\n",
        "                self.listZ.append(val)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fileName\n",
        "\n",
        "class ParsedData:\n",
        "    def __init__(self, dict_json: dict):\n",
        "        self.fileName = dict_json['fileName']\n",
        "        self.motion = dict_json['motion']\n",
        "        self.time = dict_json['time']\n",
        "        self.userName = dict_json['userName']\n",
        "        self.listX = dict_json['listX']\n",
        "        self.listY = dict_json['listY']\n",
        "        self.listZ = dict_json['listZ']\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fileName     \n",
        "      \n",
        "class CroppedData:\n",
        "    def __init__(self, file_name, motion, time, user_name, list_x, list_y, list_z):\n",
        "        self.fileName = file_name\n",
        "        self.motion = motion\n",
        "        self.time = time\n",
        "        self.userName = user_name\n",
        "        self.listX = list_x\n",
        "        self.listY = list_y\n",
        "        self.listZ = list_z\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.fileName\n",
        "\n",
        "class MarkedData:\n",
        "    def __init__(self, file_name, motion, time, user_name, list_x, list_y, list_z, steps):\n",
        "        self.fileName = file_name\n",
        "        self.motion = motion\n",
        "        self.time = time\n",
        "        self.userName = user_name\n",
        "        self.listX = list_x\n",
        "        self.listY = list_y\n",
        "        self.listZ = list_z\n",
        "        self.steps = steps\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.fileName"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7waZGHvzqz5",
        "colab_type": "text"
      },
      "source": [
        "# **Create directories and file list**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "849xpz4TD5GA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_folder(folder: str):\n",
        "  if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "    logging.debug(f\"A directory {folder} has been created.\")\n",
        "\n",
        "    \n",
        "def create_folders(folders: list):\n",
        "  for folder in folders:\n",
        "    create_folder(folder)\n",
        "    \n",
        "\n",
        "def list_local_files(folder: str) -> list:\n",
        "  files = [f\"{folder}/\" + file for file in os.listdir(folder)]\n",
        "  random.shuffle(files)\n",
        "  return files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8IHjXL4zvON",
        "colab_type": "text"
      },
      "source": [
        "# **FireBase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv7Z5vF9Bpge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_blobs(bucket_name: str, blob_prefix: str = None, delimiter: str = None) -> list:\n",
        "    \"\"\"\n",
        "    This function returns a list of all files in FireBase. This will be needed to download files.\n",
        "    :param bucket_name: This is the name of the repository with such ending appspot.com\n",
        "    :param blob_prefix: Directory Name\n",
        "    :param delimiter: No, youâ€™ll understand it yourself. This is a separator\n",
        "    :return: List<Blob> with fields for each blob: name, id, bucket_name.\n",
        "    \"\"\"\n",
        "\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=blob_prefix, delimiter=delimiter)\n",
        "    return [blob for blob in blobs]\n",
        "\n",
        "\n",
        "def download_blobs(blobs: list, folder: str):\n",
        "    \"\"\"\n",
        "    This function downloads each blob from the blobs list to the specified directory.\n",
        "    :param blobs: List of storage.Blob\n",
        "    :param folder: Path to save blobs(json)\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    create_folder(folder)\n",
        "\n",
        "    for blob in blobs:\n",
        "        destination_uri = f\"{folder}/{blob.name.split('/')[1]}\"\n",
        "        blob.download_to_filename(destination_uri)\n",
        "        logging.debug(f\"The file ({blob.name}) has been downloaded to the {folder}.\")\n",
        "\n",
        "\n",
        "def upload_blob(bucket_name: str, file_path, file_name: str, folder_path: str):\n",
        "    \"\"\"\n",
        "    Upload the specified file to a specific folder on FireBase.\n",
        "    :param bucket_name: This is the name of the repository with such ending appspot.com\n",
        "    :param file_path: The path to the file to be saved.\n",
        "    :param file_name: The name of the file to be displayed in FireBase.\n",
        "    :param folder_path: The name of the folder on FireBase to which the file will be saved.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    target_blob = bucket.blob(f\"{folder_path}/{file_name}\")\n",
        "    target_blob.upload_from_filename(file_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK-VtpN3xvNa",
        "colab_type": "text"
      },
      "source": [
        "# **Work with JSON Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0yfTiuRlZX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class to JSON file with random name\n",
        "def serialize_json(data, folder: str, file_name: str):\n",
        "    create_folder(folder)\n",
        "    purified_name = file_name.split(\"||\")[0]\n",
        "    full_name = f\"{purified_name}||{randint(10 ** 4, 10 ** 17)}.json\"\n",
        "    data.fileName = full_name\n",
        "\n",
        "    with open(f\"{folder}/{full_name}\", \"w\") as write_file:\n",
        "        json.dump(data.__dict__, write_file)\n",
        "\n",
        "\n",
        "# Save json from class\n",
        "def save_json(data, folder: str, file_name: str):\n",
        "    with open(f\"{folder}/{file_name}\", \"w\") as write_file:\n",
        "        json.dump(data.__dict__, write_file)\n",
        "        \n",
        "        \n",
        "# Open json and get parsed dict\n",
        "def open_json(path: str) -> dict:\n",
        "    with open(path) as json_file:\n",
        "      return json.load(json_file)\n",
        "\n",
        "\n",
        "# JSON to Data class\n",
        "def deserialize_json(file_path: str) -> Data:\n",
        "    with open(file_path) as json_file:\n",
        "        data = json.load(json_file)\n",
        "        return Data(data)\n",
        "\n",
        "\n",
        "# JSON to ParsedData class\n",
        "def deserialize_parsed_json(file_path: str) -> ParsedData:\n",
        "    with open(file_path) as json_file:\n",
        "        data = json.load(json_file)\n",
        "        return ParsedData(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0p_BVRyxGZP",
        "colab_type": "text"
      },
      "source": [
        "# **Work with local files**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aw3r6Auj-dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distribute(old_folder: str, new_folder: str, parsed: bool):\n",
        "    \"\"\"\n",
        "    Distributes all measurements by activity type, copying to a new folder.\n",
        "    :param old_folder: The folder from which files are taken.\n",
        "    :param new_folder: Folder to which distributed meas. are saved.\n",
        "    :param parsed: Are the points in our dimension divided by the coordinates?\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    \n",
        "    run_folder = f\"{new_folder}/run\"\n",
        "    jump_folder = f\"{new_folder}/jump\"\n",
        "    walk_folder = f\"{new_folder}/walk\"\n",
        "    create_folders([run_folder, jump_folder, walk_folder])\n",
        "    files = list_local_files(old_folder)\n",
        "\n",
        "    for file in files:\n",
        "        data = deserialize_parsed_json(file) if parsed else deserialize_json(file)\n",
        "\n",
        "        if (data.motion['name'].lower() == \"run\"):\n",
        "            copyfile(file, f\"{run_folder}/{data.fileName}\")\n",
        "\n",
        "        if (data.motion['name'].lower() == \"jump\"):\n",
        "            copyfile(file, f\"{jump_folder}/{data.fileName}\")\n",
        "\n",
        "        if (data.motion['name'].lower() == \"walk\"):\n",
        "            copyfile(file, f\"{walk_folder}/{data.fileName}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4HdKQcXMR-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_cut(tmp: Data, cropped_list_x, cropped_list_y, cropped_list_z, folder: str):\n",
        "    \"\"\"\n",
        "    Saving all cut measurements with exactly 300 points.\n",
        "    :param tmp: An object with already cut coordinate lists.\n",
        "    :param cropped_list_x: X-coordinate acceleration list.\n",
        "    :param cropped_list_y: Y-coordinate acceleration list.\n",
        "    :param cropped_list_z: Z-coordinate acceleration list.\n",
        "    :param folder: The path where to save the cropped files.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    for list_x, list_y, list_z in zip(cropped_list_x, cropped_list_y, cropped_list_z):\n",
        "        if (300 == len(list_x) == len(list_y) == len(list_z)):\n",
        "            cropped_data = CroppedData(tmp.fileName, tmp.motion, tmp.time, tmp.userName, list_x, list_y, list_z)\n",
        "            serialize_json(cropped_data, folder, cropped_data.fileName)\n",
        "\n",
        "\n",
        "def cut_data(folder_path: str, new_folder: str):\n",
        "    \"\"\"\n",
        "    Creating multiple measurements from one, with a given number of points.\n",
        "    :param folder_path: The folder in which the uncut files are stored.\n",
        "    :param new_folder: Folder where to save sliced measurements.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    files = list_local_files(folder_path)\n",
        "    for file in files:\n",
        "        tmp = deserialize_json(file)\n",
        "        cropped_lists_x = list(sliced(tmp.listX, 300))\n",
        "        cropped_lists_y = list(sliced(tmp.listY, 300))\n",
        "        cropped_lists_z = list(sliced(tmp.listZ, 300))\n",
        "\n",
        "        save_cut(tmp, cropped_lists_x, cropped_lists_y, cropped_lists_z, new_folder)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc4XkqykMR7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mark_up(obj, target_folder, steps: int):\n",
        "    \"\"\"\n",
        "    Forms and saves the marked measurement from obj, sends it to the server.\n",
        "    :param obj: Old unallocated measurements.\n",
        "    :param target_folder: The folder in which to save the marked up measurements.\n",
        "    :param steps: Number of steps.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    \n",
        "    create_folder(target_folder)\n",
        "    data = MarkedData(obj.fileName, obj.motion, obj.time, obj.userName, obj.listX, obj.listY, obj.listZ, steps)\n",
        "    save_json(data, target_folder, data.fileName)\n",
        "    upload_blob('activitytracker-bde5c.appspot.com', f\"{target_folder}/{data.fileName}\", data.fileName, target_folder)\n",
        "\n",
        "\n",
        "def marking_data(folder_path: str, target_folder: str):\n",
        "    \"\"\"\n",
        "    Iterates over each file, draws a graph and asks for the number of steps.\n",
        "    :param folder_path: The folder in which the unlabeled data is located.\n",
        "    :param target_folder: The folder in which to save the data.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    for file in list_local_files(folder_path):\n",
        "        parsed_data = deserialize_parsed_json(file)\n",
        "        title = f\"{parsed_data.userName} - {parsed_data.motion['name']}\"\n",
        "        x = parsed_data.listX\n",
        "        y = parsed_data.listY\n",
        "        z = parsed_data.listZ\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.figure(1)\n",
        "        plt.title(title)\n",
        "        plt.subplot(111)\n",
        "        plt.plot(x)\n",
        "        plt.ylim(-1000, 1000)\n",
        "        plt.subplot(111)\n",
        "        plt.plot(y)\n",
        "        plt.ylim(-1000, 1000)\n",
        "        plt.subplot(111)\n",
        "        plt.plot(z)\n",
        "        plt.ylim(-1000, 1000)\n",
        "        plt.show()\n",
        "        steps = int(input(\"Count steps: \"))\n",
        "        mark_up(parsed_data, file, target_folder, steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6FSGaMQqPD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balance_classes(target_folder: str):\n",
        "    \"\"\"\n",
        "    Keeps an equal number of measurements for each class.\n",
        "    :param target_folder: The path to save a balanced dataset.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(target_folder):\n",
        "      rmtree(target_folder)\n",
        "\n",
        "    create_folder(target_folder)\n",
        "\n",
        "\n",
        "    list_walk = list_local_files(\"google-cloud/dataset-cut-parted/walk\")\n",
        "    list_jump = list_local_files(\"google-cloud/dataset-cut-parted/jump\")\n",
        "    list_run = list_local_files(\"google-cloud/dataset-cut-parted/run\")\n",
        "\n",
        "    min_len = min(len(list_walk), len(list_jump), len(list_run))\n",
        "    print(f\"Min: {min_len}\")\n",
        "    list_files = list_walk[:min_len] + list_jump[:min_len] + list_run[:min_len]\n",
        "\n",
        "    print(f\"Walk: {len(list_walk)}\")\n",
        "    print(f\"Jump: {len(list_jump)}\")\n",
        "    print(f\"Run: {len(list_run)}\")\n",
        "    print(f\"Balanced dataset: {len(list_files)}\")\n",
        "    for file in list_files:\n",
        "        data = deserialize_parsed_json(file)\n",
        "        copyfile(file, f\"{target_folder}/{data.fileName}\")\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KCiyMhlxbfa",
        "colab_type": "text"
      },
      "source": [
        "#**Loading and balancing dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WzpQlnBsA2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download all file from FireBase\n",
        "list_blobs = get_blobs('activitytracker-bde5c.appspot.com', \"measurements\")\n",
        "download_blobs(list_blobs, \"google-cloud/measurements\")\n",
        "\n",
        "# Distribute by motions\n",
        "distribute(\"google-cloud/measurements\", \"google-cloud/dataset\", False)\n",
        "\n",
        "# Cropping Data and distribute\n",
        "cut_data(\"google-cloud/measurements\", \"google-cloud/dataset-cut\")\n",
        "distribute(\"google-cloud/dataset-cut\", \"google-cloud/dataset-cut-parted\", True)\n",
        "\n",
        "# Balance the number of measurements in each class\n",
        "balance_classes(\"google-cloud/dataset-cut-balanced\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRwh_1-W0dOJ",
        "colab_type": "text"
      },
      "source": [
        "# **Data preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F6Me1ehmith",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parses class names into a binary representation.\n",
        "def mark_motion(name: str) -> list:\n",
        "    run = [0, 0, 1]\n",
        "    jump = [0, 1, 0]\n",
        "    walk = [1, 0, 0]\n",
        "\n",
        "    if (name.lower() == \"run\"):\n",
        "        return run\n",
        "    elif (name.lower() == \"jump\"):\n",
        "        return jump\n",
        "    elif (name.lower() == \"walk\"):\n",
        "        return walk\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz_v8IsjC_z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn three axes into one using L2 Norm\n",
        "def approximation(list_x, list_y, list_z) -> list:\n",
        "    result = np.zeros(300)\n",
        "    for i in range(0, len(list_x)):\n",
        "        result[i] = math.sqrt(list_x[i] ** 2 + list_y[i] ** 2 + list_z[i] ** 2)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRT73xTqrV8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X and y (target) loading for dataset from folder\n",
        "def load_dataset(folder: str):\n",
        "  files = list_local_files(folder)\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(0, len(files)):\n",
        "    data = open_json(files[i])\n",
        "    result = approximation(data['listX'], data['listY'], data['listZ'])\n",
        "    x.append(result)\n",
        "    y.append(mark_motion(data['motion']['name']))\n",
        "  x = np.array(x)\n",
        "  x = np.reshape(x, (x.shape[0], x.shape[1], 1))\n",
        "  y = np.array(y)\n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNgiaOSC0sRo",
        "colab_type": "text"
      },
      "source": [
        "# **Model training - LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmvt_lcAxsio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset():\n",
        "  X, y = load_dataset(\"google-cloud/dataset-cut-balanced\")\n",
        "  return train_test_split(X, y, test_size=0.33, random_state=91)\n",
        "\n",
        "X_train, X_test, y_train, y_test = split_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9nT_C9R9fWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self, learning_rate, n_timesteps, n_features, n_outputs):\n",
        "    self.model = Sequential()\n",
        "    self.model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
        "    self.model.add(Dropout(0.5))\n",
        "#     self.model.add(Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'))\n",
        "#     self.model.add(Dropout(0.5))\n",
        "#     self.model.add(Flatten())\n",
        "    self.model.add(Dense(100, activation='relu'))\n",
        "    self.model.add(Dropout(0.5))\n",
        "    self.model.add(Dense(100, activation='relu'))\n",
        "    self.model.add(Dropout(0.5))\n",
        "    self.model.add(Dense(n_outputs, activation='softmax'))\n",
        "    self.model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
        "  \n",
        "  def get_model(self):\n",
        "    return self.model\n",
        "  def get_summary(self):\n",
        "    return self.model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MannPYZdGcvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from tensorflow.python.tools import freeze_graph\n",
        "from tensorflow.python.tools import optimize_for_inference_lib\n",
        "\n",
        "\n",
        "def export_model(saver, input_names, output_names, model_name='mnist_convnet'):\n",
        "    tf.train.write_graph(K.get_session().graph_def, 'out',\n",
        "                         model_name + '_graph.pbtxt')\n",
        "\n",
        "    saver.save(K.get_session(), 'out/' + model_name + '.chkp')\n",
        "\n",
        "    freeze_graph.freeze_graph('out/' + model_name + '_graph.pbtxt', None,\n",
        "                              False, 'out/' + model_name + '.chkp', output_names[0],\n",
        "                              \"save/restore_all\", \"save/Const:0\",\n",
        "                              'out/frozen_' + model_name + '.pb', True, \"\")\n",
        "\n",
        "    input_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.Open('out/frozen_' + model_name + '.pb', \"rb\") as f:\n",
        "        input_graph_def.ParseFromString(f.read())\n",
        "\n",
        "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
        "        input_graph_def, input_names, output_names,\n",
        "        tf.float32.as_datatype_enum)\n",
        "\n",
        "    with tf.gfile.FastGFile('out/opt_' + model_name + '.pb', \"wb\") as f:\n",
        "        f.write(output_graph_def.SerializeToString())\n",
        "\n",
        "    print(\"graph saved!\")\n",
        "    \n",
        "export_model(tf.train.Saver(), [\"lstm_6_input\"], [\"dense_9/Softmax\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYcMV5wqW_Ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import lite\n",
        "converter = lite.TFLiteConverter.from_keras_model_file('2019-08-22_13_21_30 (1).h5')\n",
        "tfmodel = converter.convert()\n",
        "open(\"model.tflite\" , \"wb\").write(tfmodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpJoSgazORkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "model = load_model(\"2019-08-22_13_21_30 (1).h5\")\n",
        "model.summary()\n",
        "print(model.layers[0].name)\n",
        "sess = K.get_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxj2t3VNPz7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjfzMNqtKDW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes the state of a session into a pruned computation graph.\n",
        "\n",
        "    Creates a new computation graph where variable nodes are replaced by\n",
        "    constants taking their current value in the session. The new graph will be\n",
        "    pruned so subgraphs that are not necessary to compute the requested\n",
        "    outputs are removed.\n",
        "    @param session The TensorFlow session to be frozen.\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\n",
        "                          or None to freeze all the variables in the graph.\n",
        "    @param output_names Names of the relevant graph outputs.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = ''\n",
        "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "            session, input_graph_def, output_names, freeze_var_names)\n",
        "        return frozen_graph\n",
        "\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]], 'float32')\n",
        "Y = np.array([[0], [1], [1], [0]], 'float32')\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(64, input_dim=2, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
        "\n",
        "model.fit(X, Y, batch_size=1, nb_epoch=100, verbose=0)\n",
        "\n",
        "# inputs:  ['dense_input']\n",
        "print('inputs: ', [input.op.name for input in model.inputs])\n",
        "\n",
        "# outputs:  ['dense_4/Sigmoid']\n",
        "print('outputs: ', [output.op.name for output in model.outputs])\n",
        "\n",
        "model.save('./xor.h5')\n",
        "\n",
        "frozen_graph = freeze_session(tf.keras.backend.get_session(), output_names=[out.op.name for out in model.outputs])\n",
        "tf.train.write_graph(frozen_graph, './', 'xor.pbtxt', as_text=True)\n",
        "tf.train.write_graph(frozen_graph, './', 'xor.pb', as_text=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxQYgK_Atjgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_model = Model(0.001, 300, 1, 3)\n",
        "# lstm model\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  print(trainX.shape, trainy.shape)\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  \n",
        "  # comile network\n",
        "  model = custom_model.get_model()\n",
        "  \n",
        "  # fit network\n",
        "  history = model.fit(trainX, trainy, epochs=300, validation_split=0.2, shuffle=True)\n",
        "  \n",
        "  # graphics   \n",
        "  f = plt.figure(figsize=(20,6))\n",
        "  acc = f.add_subplot(121)\n",
        "  loss = f.add_subplot(122)\n",
        "  acc.plot(history.history['acc'])\n",
        "  acc.plot(history.history['val_acc'])\n",
        "  acc.set_title('model accuracy')\n",
        "  acc.set_ylabel('accuracy')\n",
        "  acc.set_xlabel('epoch')\n",
        "  acc.legend(['train', 'val'], loc='upper left')\n",
        "  loss.plot(history.history['loss'])\n",
        "  loss.plot(history.history['val_loss'])\n",
        "  loss.set_title('model loss')\n",
        "  loss.set_ylabel('loss')\n",
        "  loss.set_xlabel('epoch')\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "  \n",
        "  # evaluate model and save\n",
        "  _, accuracy = model.evaluate(testX, testy, verbose=0)\n",
        "  round_acc = int(accuracy*100) \n",
        "  model.save(f\"{round_acc}_{datetime.now().strftime('%m-%d_%H:%M:%S')}.h5\")\n",
        "  return accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats, X_tr, X_te, y_tr, y_te):\n",
        "  \n",
        "  # repeat experiment\n",
        "  scores = list()\n",
        "  for repeat in range(repeats):\n",
        "    score = evaluate_model(X_tr, y_tr, X_te, y_te)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (repeat+1, score))\n",
        "    scores.append(score)\n",
        "    \n",
        "  # summarize results\n",
        "  summarize_results(scores)\n",
        "\n",
        "# run the experiment\n",
        "run_experiment(3, X_train, X_test, y_train, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IZBFhWQw6aK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(model, testX, testy):\n",
        "  _, accuracy = model.evaluate(testX, testy, verbose=1)\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djfaMpQI05gz",
        "colab_type": "text"
      },
      "source": [
        "# **Delete all data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFdCikisZD94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check = input(\"Press F to delete all data: \")\n",
        "\n",
        "rmtree('google-cloud') if check == \"F\" else print(\"I need to get away!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BcKXXVm3pXn",
        "colab_type": "text"
      },
      "source": [
        "# **Predict classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvoNmN4b3ti7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = load_dataset(\"test-jump\")\n",
        "ynew = model.predict_classes(X)\n",
        "# show the inputs and predicted outputs\n",
        "for res in ynew:\n",
        "  print(\"Predicted=%s\" % res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzTrO2ZQN6Xq",
        "colab_type": "text"
      },
      "source": [
        "# Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMOfQZAmLxp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def confusion_matrix(model, n_classes=3):\n",
        "  X, y = load_dataset(\"google-cloud/dataset-cut-balanced\")\n",
        "  assert X.shape[0] == y.shape[0]\n",
        "  assert n_classes == y.shape[1]\n",
        "  confusion = np.zeros((n_classes, n_classes))\n",
        "  \n",
        "  for k in tqdm(range(X.shape[0])):\n",
        "    y_hat = model.predict(np.expand_dims(X[k], axis=0))\n",
        "    predicted = np.argmax(y_hat[0])\n",
        "    correct = np.argmax(y[k])\n",
        "    confusion[correct][predicted] += 1.\n",
        "    \n",
        "    \n",
        "  df_cm = pd.DataFrame(confusion, index = [\"Walk\",\"Jump\", \"Run\"], columns = [\"Walk\",\"Jump\", \"Run\"])\n",
        "  plt.figure(figsize = (10,7))\n",
        "  sn.heatmap(df_cm, annot=True)\n",
        "  plt.xlabel(\"Predicted\")\n",
        "  plt.ylabel(\"Correct\")\n",
        "\n",
        "    \n",
        "    \n",
        "  return confusion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf__uD3JOLTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_matrix = confusion_matrix(custom_model.get_model())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tts50s25RiB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ_U2GtPoNGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(n_timestamps, n_features, n_classes):\n",
        "  model = Sequential([\n",
        "       LSTM(units=100, input_shape=(n_timestamps, n_features), return_sequences=True),\n",
        "       Dropout(0.5),\n",
        "       Conv1D(filters=64, kernel_size=9, activation='relu', padding='same'),\n",
        "       Dropout(0.5),\n",
        "       Conv1D(filters=16, kernel_size=9, activation='relu', padding='same'),\n",
        "       Dropout(0.5),\n",
        "       Flatten(),\n",
        "       Dense(units=100, activation='relu'),\n",
        "       Dropout(0.5),\n",
        "       Dense(units=n_classes, activation='softmax'),\n",
        "   ])\n",
        "  model.compile(optimizer=Adam(), metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}